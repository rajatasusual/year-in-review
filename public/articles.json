{
    "status": "ok",
    "feed": {
        "url": "https://medium.com/feed/@rajatasusual",
        "title": "Stories by Rajat on Medium",
        "link": "https://medium.com/@rajatasusual?source=rss-07ca2cd9b1d2------2",
        "author": "",
        "description": "Stories by Rajat on Medium",
        "image": "https://cdn-images-1.medium.com/fit/c/150/150/1*xmvbdiLQ-1r-SK_C7_TVHg.jpeg"
    },
    "items": [
        {
            "title": "Gemini-MUD: A Technical Exploration of AI-Powered Procedural Generation",
            "pubDate": "2024-08-14 17:30:01",
            "link": "https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/6b52186361d0",
            "author": "Rajat",
            "thumbnail": "",
            "description": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*U2TwLHzU2C4VwVeC8RvdiQ.png\" width=\"2940\"></a></p>\n<p class=\"medium-feed-snippet\">In our quest to unravel the mysteries of AI’s capabilities, we’ve previously witnessed language models engage in a playful battle of wits…</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa »</a></p>\n</div>",
            "content": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*U2TwLHzU2C4VwVeC8RvdiQ.png\" width=\"2940\"></a></p>\n<p class=\"medium-feed-snippet\">In our quest to unravel the mysteries of AI’s capabilities, we’ve previously witnessed language models engage in a playful battle of wits…</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa »</a></p>\n</div>",
            "enclosure": {},
            "categories": [
                "generative-ai-tools",
                "game-development",
                "generative-art",
                "artificial-intelligence",
                "llm"
            ]
        },
        {
            "title": "When AI’s Generative Spirit Meets the Rigour of System Design",
            "pubDate": "2024-08-14 17:21:53",
            "link": "https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/85eb0ee218f2",
            "author": "Rajat",
            "thumbnail": "",
            "description": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*WUUdoGN6Fpy8kocz\" width=\"7952\"></a></p>\n<p class=\"medium-feed-snippet\">In the spirit of turning the tables on AI, much like my previous exploration into the playful “Tur(n)ing the Tables” experiment, I now…</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa »</a></p>\n</div>",
            "content": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*WUUdoGN6Fpy8kocz\" width=\"7952\"></a></p>\n<p class=\"medium-feed-snippet\">In the spirit of turning the tables on AI, much like my previous exploration into the playful “Tur(n)ing the Tables” experiment, I now…</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa »</a></p>\n</div>",
            "enclosure": {},
            "categories": [
                "generative-ai-use-cases",
                "procedural-generation",
                "artificial-intelligence",
                "llm",
                "gaming"
            ]
        },
        {
            "title": "Tur(n)ing the Tables — Making Language Models Fight.",
            "pubDate": "2024-07-30 08:02:59",
            "link": "https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/cfa0bc168878",
            "author": "Rajat",
            "thumbnail": "",
            "description": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*fOe2oo0kSm3pFJsGAttYAw.png\" width=\"2608\"></a></p>\n<p class=\"medium-feed-snippet\">A fun take on a famous experiment, the Turing Test, and with a similar objective — how test how smart (or deceptive) a machine is. The…</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa »</a></p>\n</div>",
            "content": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*fOe2oo0kSm3pFJsGAttYAw.png\" width=\"2608\"></a></p>\n<p class=\"medium-feed-snippet\">A fun take on a famous experiment, the Turing Test, and with a similar objective — how test how smart (or deceptive) a machine is. The…</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa »</a></p>\n</div>",
            "enclosure": {},
            "categories": [
                "game-development",
                "language-model",
                "technology",
                "data-science",
                "artificial-intelligence"
            ]
        },
        {
            "title": "Bedrock of Language Models — Vector Embeddings, Visualised.",
            "pubDate": "2024-07-20 19:50:13",
            "link": "https://medium.com/taming-the-llama/bedrock-of-language-models-vector-embeddings-visualised-a83fcb6209af?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/a83fcb6209af",
            "author": "Rajat",
            "thumbnail": "",
            "description": "\n<h3>Bedrock of Language Models — Vector Embeddings, Visualised.</h3>\n<figure><img alt=\"Vector Embeddings Smoothened using Moving averages plotted on 2D space\" src=\"https://cdn-images-1.medium.com/max/800/1*8m_5HMn0THG402smMTCJuA.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings.git\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><blockquote>The article demonstrates how to understand the way NLP systems breakdown content into vectors into multi-dimensional space and then connect the dots. It is incredibly complex for a human brain to visualise higher dimensional spaces but for machines it is easy business. So, I created a simple yet what I believe is a novel solution to help us comprehend this concept.</blockquote>\n<blockquote>check out: <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a>\n</blockquote>\n<blockquote>Shout out to <a href=\"https://medium.com/u/94d7e8f6238c\">Sam Gallagher</a> and his incredible article (<a href=\"https://medium.com/@gallaghersam95/visualizing-embedding-vectors-99cac1d164c4\">Visualizing embedding vectors</a>) for the inspiration.</blockquote>\n<h3>Introduction</h3>\n<p>Vectors are fundamental to modern Natural Language Processing (NLP) and by extension to Large Language Models (LLMs). They are a means to represent text (or any context) in a numerical format that machines can process. The definition of the vector remains the same that we remember from high school —</p>\n<blockquote>A geometric object that has a length and magnitude in n dimensional space.</blockquote>\n<p>Now, An <strong>embedding model </strong>is a special algorithm that is designed to put each word, each sentence into these dimensions— for simpler terms, these dimensions can be thought of as attibutes. Even the simplest of these embedding models classify each word into ~1000 dimensions.</p>\n<p><em>How does that look mathematically?</em> Well, it’s simple. It is just a an array of say 1000 numbers where each number ranges from -x to x where x represents related to that particular dimension or attribute. That’s how a machine understands similarity and draws semantic meaning from the input.</p>\n<p>By capturing semantic meanings in a high-dimensional space, vectors allow models to understand and manipulate language in sophisticated ways. But you know it is just mathematics at the end of it — geometry and statistics.</p>\n<h4>Basic groundwork to understand vector embeddings further</h4>\n<p>Consider two sentences:<br>1. “The cat sits on the mat.”<br>2. “A dog lies on the rug.”</p>\n<p>An embedding model doesn’t care what a dog or a cat is but because it is trained on such a huge corpus of data, it knows cat or dog belong in somewhat similar category (without knowing the concept of what “animals” are). So it converts the above sentences into numerical arrays, vectors. For simplicity, let’s assume the vectors for these sentences are:<br>- Sentence 1: `[0.1, 0.2, 0.3, 0.4, 0.5]`<br>- Sentence 2: `[0.2, 0.1, 0.4, 0.3, 0.5]`</p>\n<p>These vectors encapsulate the semantic meaning of the sentences in a multi-dimensional space, in this case a 5 dimension space. The closer these vectors are, the more similar the sentences are considered to be.</p>\n<p>Consider another sentence:</p>\n<p>3. “Sun is shining, flowers are blooming”.</p>\n<p>It might be converted to a vector like [-0.7, 2.2, 1.3, -2.7, 4.5].</p>\n<p>Another one like:</p>\n<p>4. “Horse is huge”</p>\n<p>This would be slightly more similar to the first two sentences, something like: [0.1, 1.2, -0.7, 1.7, 0.5].</p>\n<p>Now, in this example that doesn’t require a graphics processor to run, you might notice that a few of the dimensions in sentences 1,2 &amp; 4 are very, very close. That’s it. That’s how a machine would figure what you are talking about, again without knowing what an animal is, what a rug is or what sun is.</p>\n<p>We will touch how a Language model finds similarity between two texts a bit later.</p>\n<h3>The Visualization Challenge</h3>\n<h4>Why does this project even exist?</h4>\n<p>I’ve been diving into projects where embedding vectors are crucial. These vectors live in dimensions beyond our visual grasp, so we use cosine similarity to make sense of them. But, I wanted more — I wanted to see them.</p>\n<h4>A bit more…</h4>\n<p>As I have been building a comprehensive Retrieval-Augmented Generation (RAG) system, I want to visualize the output I have been receiving by machine into something my low on GPU power brain can process.</p>\n<p>While it’s cool that math and tech can tell us this, I wanted to actually see these vectors. It’s one thing for math to say they’re similar; it’s another to see it with our own eyes.</p>\n<h3>Let’s Get into it</h3>\n<p>The premise is simple. We use an embedding model to convert a text input and map it on a 2 dimensional axis.</p>\n<figure><img alt=\"Vector embedding plotted on 2D with x axis as dimension and y, the magnitude.\" src=\"https://cdn-images-1.medium.com/max/800/1*u4DFZoXYHXSSuTAgxsdAPA.png\"><figcaption>vector embedding generated by nomic-embed-text model for word “test” mapped in 2D space.</figcaption></figure><p>Not very clean, is it? Brace yourself when I tell you that the embedding model spit this out only for the input of “Test”. 700+ dimensions!</p>\n<p>Draw an analogy to a 3 dimensional vector as follows:</p>\n<figure><img alt=\"source: https://www.intmath.com/vectors/7-vectors-in-3d-space.php\" src=\"https://cdn-images-1.medium.com/max/313/1*hKyMhar0H0DzyQBoNwh2rA.png\"><figcaption>A simple 3 dimensional vector</figcaption></figure><p>Now let’s see how two text inputs which are converted to vectors look like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*3QTB5_gWRVWHcOi0rSPBlg.png\"><figcaption>vector embeddings generated by nomic-embed-text model for words “test” and “exam” mapped in 2D space.</figcaption></figure><p>Still doesn’t make sense on what we are looking at or if the inputs are similar or not, does it? Let’s see how a machine compares two inputs.</p>\n<h4>Cosine Similarity</h4>\n<p>Cosine similarity is by far the most common way a machine identifies whether two inputs are related. It is a metric used to measure how similar two vectors are, based on the cosine of the angle between them. It ranges from -1 (completely dissimilar) to 1 (completely similar), surprisingly the same range of any cosine angle. The math is simple. here it is.</p>\n<h4>Calculation</h4>\n<p>Here’s the code that forms the bedrock LLMs retrieval (it’s simplified in case you were wondering):</p>\n<pre>function calculateCosineSimilarity(vec1, vec2) {<br> const dotProduct = vec1.reduce((acc, val, idx) =&gt; acc + val * vec2[idx], 0);<br> const magnitude1 = Math.sqrt(vec1.reduce((acc, val) =&gt; acc + val * val, 0));<br> const magnitude2 = Math.sqrt(vec2.reduce((acc, val) =&gt; acc + val * val, 0));<br> return dotProduct / (magnitude1 * magnitude2);<br>}<br><br>// Example vectors<br>const vectorA = [1, 2, 3];<br>const vectorB = [4, 5, 6];<br>const similarity = calculateCosineSimilarity(vectorA, vectorB);<br>console.log(`Cosine Similarity: ${similarity.toFixed(2)}`); // Output: Cosine Similarity: 0.97</pre>\n<p>But we are still stuck in understanding how to figure out visually if two text inputs are similar. So, we will simplify the machine input and here comes:</p>\n<h4>The Need for Smoothing</h4>\n<p>When visualizing vector embeddings, especially with large datasets, the graph can become cluttered and noisy, as we saw above.</p>\n<blockquote>Smoothing helps by averaging values over a specified window, reducing noise and making the graph more interpretable.</blockquote>\n<h4>Why Smoothing?</h4>\n<p>Smoothing is crucial for:<br>1. <strong>Clarity</strong>: Reduces the visual noise, making patterns more evident.<br>2. <strong>Interpretation</strong>: Helps in identifying trends and relationships that may be obscured by raw data points.</p>\n<h4>Implementing Smoothing</h4>\n<p>The smoothing function helps in reducing the noise in the data by averaging the values over a specified window size. This makes the graph more readable and allows for better visualization of the overall trends in the data.</p>\n<pre>const smoothData = (data, windowSize) =&gt; {<br>    const smoothed = [];<br>    for (let i = 0; i &lt; data.length; i++) {<br>        const start = Math.max(0, i - Math.floor(windowSize / 2));<br>        const end = Math.min(data.length, i + Math.floor(windowSize / 2) + 1);<br>        const window = data.slice(start, end);<br>        const average = window.reduce((sum, val) =&gt; sum + val, 0) / window.length;<br>        smoothed.push(average);<br>    }<br>    return smoothed;<br>};</pre>\n<p>And then when you visualize the graph, it looks something like this:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*gsh8zoGGk2CaM_OmoCtpUQ.png\"></figure><p>Easier to see which inputs are similar, correct? I rest my case.</p>\n<h3>Application Setup</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*plRshCNiLEkAfwgpqT12jw.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><p>The entire application is open sourced <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></p>\n<p>You will find that it has CLI and UI modes and straightforward instructions to set up. It runs completely locally.</p>\n<p>Are you curious to explore more about how to learn more about LLMs being run locally with your own data, securely? Read this: <a href=\"https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4</a></p>\n<blockquote>Follow: <a href=\"https://github.com/rajatasusual/llamapp\">https://github.com/rajatasusual/llamapp</a> to know more.</blockquote>\n<h3>Final Thoughts</h3>\n<p>Vectors play a pivotal role in NLP and LLMs by providing a numerical representation of text. Visualizing these vectors is challenging but crucial for understanding the relationships between different pieces of text. I have tried to simplify this by providing an interactive tool to visualize and comprehend vector embeddings. Try it out and see how text relates in multi-dimensional space!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a83fcb6209af\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/bedrock-of-language-models-vector-embeddings-visualised-a83fcb6209af\">Bedrock of Language Models — Vector Embeddings, Visualised.</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "content": "\n<h3>Bedrock of Language Models — Vector Embeddings, Visualised.</h3>\n<figure><img alt=\"Vector Embeddings Smoothened using Moving averages plotted on 2D space\" src=\"https://cdn-images-1.medium.com/max/800/1*8m_5HMn0THG402smMTCJuA.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings.git\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><blockquote>The article demonstrates how to understand the way NLP systems breakdown content into vectors into multi-dimensional space and then connect the dots. It is incredibly complex for a human brain to visualise higher dimensional spaces but for machines it is easy business. So, I created a simple yet what I believe is a novel solution to help us comprehend this concept.</blockquote>\n<blockquote>check out: <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a>\n</blockquote>\n<blockquote>Shout out to <a href=\"https://medium.com/u/94d7e8f6238c\">Sam Gallagher</a> and his incredible article (<a href=\"https://medium.com/@gallaghersam95/visualizing-embedding-vectors-99cac1d164c4\">Visualizing embedding vectors</a>) for the inspiration.</blockquote>\n<h3>Introduction</h3>\n<p>Vectors are fundamental to modern Natural Language Processing (NLP) and by extension to Large Language Models (LLMs). They are a means to represent text (or any context) in a numerical format that machines can process. The definition of the vector remains the same that we remember from high school —</p>\n<blockquote>A geometric object that has a length and magnitude in n dimensional space.</blockquote>\n<p>Now, An <strong>embedding model </strong>is a special algorithm that is designed to put each word, each sentence into these dimensions— for simpler terms, these dimensions can be thought of as attibutes. Even the simplest of these embedding models classify each word into ~1000 dimensions.</p>\n<p><em>How does that look mathematically?</em> Well, it’s simple. It is just a an array of say 1000 numbers where each number ranges from -x to x where x represents related to that particular dimension or attribute. That’s how a machine understands similarity and draws semantic meaning from the input.</p>\n<p>By capturing semantic meanings in a high-dimensional space, vectors allow models to understand and manipulate language in sophisticated ways. But you know it is just mathematics at the end of it — geometry and statistics.</p>\n<h4>Basic groundwork to understand vector embeddings further</h4>\n<p>Consider two sentences:<br>1. “The cat sits on the mat.”<br>2. “A dog lies on the rug.”</p>\n<p>An embedding model doesn’t care what a dog or a cat is but because it is trained on such a huge corpus of data, it knows cat or dog belong in somewhat similar category (without knowing the concept of what “animals” are). So it converts the above sentences into numerical arrays, vectors. For simplicity, let’s assume the vectors for these sentences are:<br>- Sentence 1: `[0.1, 0.2, 0.3, 0.4, 0.5]`<br>- Sentence 2: `[0.2, 0.1, 0.4, 0.3, 0.5]`</p>\n<p>These vectors encapsulate the semantic meaning of the sentences in a multi-dimensional space, in this case a 5 dimension space. The closer these vectors are, the more similar the sentences are considered to be.</p>\n<p>Consider another sentence:</p>\n<p>3. “Sun is shining, flowers are blooming”.</p>\n<p>It might be converted to a vector like [-0.7, 2.2, 1.3, -2.7, 4.5].</p>\n<p>Another one like:</p>\n<p>4. “Horse is huge”</p>\n<p>This would be slightly more similar to the first two sentences, something like: [0.1, 1.2, -0.7, 1.7, 0.5].</p>\n<p>Now, in this example that doesn’t require a graphics processor to run, you might notice that a few of the dimensions in sentences 1,2 &amp; 4 are very, very close. That’s it. That’s how a machine would figure what you are talking about, again without knowing what an animal is, what a rug is or what sun is.</p>\n<p>We will touch how a Language model finds similarity between two texts a bit later.</p>\n<h3>The Visualization Challenge</h3>\n<h4>Why does this project even exist?</h4>\n<p>I’ve been diving into projects where embedding vectors are crucial. These vectors live in dimensions beyond our visual grasp, so we use cosine similarity to make sense of them. But, I wanted more — I wanted to see them.</p>\n<h4>A bit more…</h4>\n<p>As I have been building a comprehensive Retrieval-Augmented Generation (RAG) system, I want to visualize the output I have been receiving by machine into something my low on GPU power brain can process.</p>\n<p>While it’s cool that math and tech can tell us this, I wanted to actually see these vectors. It’s one thing for math to say they’re similar; it’s another to see it with our own eyes.</p>\n<h3>Let’s Get into it</h3>\n<p>The premise is simple. We use an embedding model to convert a text input and map it on a 2 dimensional axis.</p>\n<figure><img alt=\"Vector embedding plotted on 2D with x axis as dimension and y, the magnitude.\" src=\"https://cdn-images-1.medium.com/max/800/1*u4DFZoXYHXSSuTAgxsdAPA.png\"><figcaption>vector embedding generated by nomic-embed-text model for word “test” mapped in 2D space.</figcaption></figure><p>Not very clean, is it? Brace yourself when I tell you that the embedding model spit this out only for the input of “Test”. 700+ dimensions!</p>\n<p>Draw an analogy to a 3 dimensional vector as follows:</p>\n<figure><img alt=\"source: https://www.intmath.com/vectors/7-vectors-in-3d-space.php\" src=\"https://cdn-images-1.medium.com/max/313/1*hKyMhar0H0DzyQBoNwh2rA.png\"><figcaption>A simple 3 dimensional vector</figcaption></figure><p>Now let’s see how two text inputs which are converted to vectors look like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*3QTB5_gWRVWHcOi0rSPBlg.png\"><figcaption>vector embeddings generated by nomic-embed-text model for words “test” and “exam” mapped in 2D space.</figcaption></figure><p>Still doesn’t make sense on what we are looking at or if the inputs are similar or not, does it? Let’s see how a machine compares two inputs.</p>\n<h4>Cosine Similarity</h4>\n<p>Cosine similarity is by far the most common way a machine identifies whether two inputs are related. It is a metric used to measure how similar two vectors are, based on the cosine of the angle between them. It ranges from -1 (completely dissimilar) to 1 (completely similar), surprisingly the same range of any cosine angle. The math is simple. here it is.</p>\n<h4>Calculation</h4>\n<p>Here’s the code that forms the bedrock LLMs retrieval (it’s simplified in case you were wondering):</p>\n<pre>function calculateCosineSimilarity(vec1, vec2) {<br> const dotProduct = vec1.reduce((acc, val, idx) =&gt; acc + val * vec2[idx], 0);<br> const magnitude1 = Math.sqrt(vec1.reduce((acc, val) =&gt; acc + val * val, 0));<br> const magnitude2 = Math.sqrt(vec2.reduce((acc, val) =&gt; acc + val * val, 0));<br> return dotProduct / (magnitude1 * magnitude2);<br>}<br><br>// Example vectors<br>const vectorA = [1, 2, 3];<br>const vectorB = [4, 5, 6];<br>const similarity = calculateCosineSimilarity(vectorA, vectorB);<br>console.log(`Cosine Similarity: ${similarity.toFixed(2)}`); // Output: Cosine Similarity: 0.97</pre>\n<p>But we are still stuck in understanding how to figure out visually if two text inputs are similar. So, we will simplify the machine input and here comes:</p>\n<h4>The Need for Smoothing</h4>\n<p>When visualizing vector embeddings, especially with large datasets, the graph can become cluttered and noisy, as we saw above.</p>\n<blockquote>Smoothing helps by averaging values over a specified window, reducing noise and making the graph more interpretable.</blockquote>\n<h4>Why Smoothing?</h4>\n<p>Smoothing is crucial for:<br>1. <strong>Clarity</strong>: Reduces the visual noise, making patterns more evident.<br>2. <strong>Interpretation</strong>: Helps in identifying trends and relationships that may be obscured by raw data points.</p>\n<h4>Implementing Smoothing</h4>\n<p>The smoothing function helps in reducing the noise in the data by averaging the values over a specified window size. This makes the graph more readable and allows for better visualization of the overall trends in the data.</p>\n<pre>const smoothData = (data, windowSize) =&gt; {<br>    const smoothed = [];<br>    for (let i = 0; i &lt; data.length; i++) {<br>        const start = Math.max(0, i - Math.floor(windowSize / 2));<br>        const end = Math.min(data.length, i + Math.floor(windowSize / 2) + 1);<br>        const window = data.slice(start, end);<br>        const average = window.reduce((sum, val) =&gt; sum + val, 0) / window.length;<br>        smoothed.push(average);<br>    }<br>    return smoothed;<br>};</pre>\n<p>And then when you visualize the graph, it looks something like this:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*gsh8zoGGk2CaM_OmoCtpUQ.png\"></figure><p>Easier to see which inputs are similar, correct? I rest my case.</p>\n<h3>Application Setup</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*plRshCNiLEkAfwgpqT12jw.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><p>The entire application is open sourced <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></p>\n<p>You will find that it has CLI and UI modes and straightforward instructions to set up. It runs completely locally.</p>\n<p>Are you curious to explore more about how to learn more about LLMs being run locally with your own data, securely? Read this: <a href=\"https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4</a></p>\n<blockquote>Follow: <a href=\"https://github.com/rajatasusual/llamapp\">https://github.com/rajatasusual/llamapp</a> to know more.</blockquote>\n<h3>Final Thoughts</h3>\n<p>Vectors play a pivotal role in NLP and LLMs by providing a numerical representation of text. Visualizing these vectors is challenging but crucial for understanding the relationships between different pieces of text. I have tried to simplify this by providing an interactive tool to visualize and comprehend vector embeddings. Try it out and see how text relates in multi-dimensional space!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a83fcb6209af\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/bedrock-of-language-models-vector-embeddings-visualised-a83fcb6209af\">Bedrock of Language Models — Vector Embeddings, Visualised.</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "enclosure": {},
            "categories": [
                "nlp",
                "visualization",
                "llm",
                "artificial-intelligence",
                "data-science"
            ]
        },
        {
            "title": "Tame Artificial Intelligence, from your laptop",
            "pubDate": "2024-07-18 17:41:59",
            "link": "https://medium.com/taming-the-llama/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/d91d3d38b9b4",
            "author": "Rajat",
            "thumbnail": "",
            "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PbGZF4nbTbblAmAijk7Kdw.png\"><figcaption>llamapp beta. (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a>)</figcaption></figure><blockquote>This is a personal project (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a> ) where I explore the extents of layer 2 solutions on top of AI models all run locally — contextually aware, no hallucinations and secure.(Shoutout to <strong>Langchain</strong>(<a href=\"https://github.com/langchain-ai\">https://github.com/langchain-ai</a>), a really cool company enabling the “powered by AI” era. P.S. the actual powered by AI and not just a prompt hiding behind a beautiful UI)</blockquote>\n<p>You know the feeling when you get a new laptop and you immediately want to see what breaks it? No? Maybe it’s me.</p>\n<p>Well turns out, running a super powerful (yet tiny by AI standards) <a href=\"https://ai.google.dev/gemma?trk=article-ssr-frontend-pulse_little-text-block\">Google Gemma</a> , a vector DB (<a href=\"http://redis.io/\">http://redis.io</a>) and a local server doesn’t. And that it gives far better results than expected.</p>\n<h3>The Elevator Pitch</h3>\n<blockquote><em>If you are methodical and pragmatic like me, the accuracy and reliability of AI responses are paramount. Many commercial AI solutions can and do produce hallucinations-answers that seem plausible but are incorrect. Additionally, the concerns about data privacy and security when using cloud-based solutions might make you want to avoid using personal context. To address these issues, I developed the Retrieval Augmented Generator (RAG) application designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</em></blockquote>\n<h3>Why Run Local and What Does It Take?</h3>\n<p>Running AI locally has significant advantages, including enhanced data privacy, control, and speed. Surprisingly, it doesn’t require a high-end setup. I ran this project on a MacBook Air M3, proving that a mid-range laptop is sufficient. This setup ensures that all data stays on your machine, giving you complete control over your information. More importantly, power to tame the AI with context and knowledge from your domain.</p>\n<h3>Understanding RAG in Simple Terms</h3>\n<p>So, what is RAG? Imagine you ask a question and receive an answer that’s not just generated by an AI but backed by verified documents. RAG works in two steps:</p>\n<p><strong>1. Retrieving Relevant Documents: </strong>It searches a database of reliable documents to find information related to your query.</p>\n<p><strong>2. Generating Responses: </strong>Using AI, it crafts a response based on the retrieved information, ensuring the answer is accurate and contextually relevant.</p>\n<p>This approach offers significant benefits over commercial AI apps, which may not always provide traceable answers or may store your data on external servers.</p>\n<p>That is just it in a nutshell — but the extent of RAG is tremendous. Chaining and combining responses with your tools of choice, executing entire workflows even.</p>\n<h3>Key Features of the RAG Engine( I named it Llamapp)</h3>\n<p>Here’s a quick overview of the standout features of the RAG Engine:</p>\n<p><strong>- Rewriting </strong>: Refines user queries to avoid distractions and enhance accuracy.</p>\n<p><strong>- Fusion Reciprocal Ranking </strong>: Combines results from multiple queries to improve response relevance. Think of google page ranking but tuned for you.</p>\n<p><strong>- Custom Loaders and Retrievers </strong>: Ensures the system can handle various document types and retrieve the most pertinent information.</p>\n<p>I am still exploring the need and various ways we can use this. I want to share this project with the community and invite you to contribute, explore, and collaborate. Whether you’re an AI enthusiast, a developer, or someone curious about the potential of local AI, your input and contributions are invaluable.</p>\n<p><strong><em>Check out the project on GitHub: </em></strong><a href=\"https://github.com/rajatasusual/llamapp?trk=article-ssr-frontend-pulse_little-text-block\"><strong><em>[GitHub link]</em></strong></a></p>\n<p>Let’s connect, share insights, and build something amazing together!</p>\n<p>This project is entirely open source, underscoring my commitment to transparency, collaboration, and shared learning. Dive into the code, tweak it, improve it, and let’s push the boundaries of what’s possible with AI!</p>\n<p>#AI #MachineLearning #Langchain #Redis #Ollama #InformationRetrieval #LearningJourney #OpenSource #GitHubProject #TechInnovation #LinkedInArticle</p>\n<p><em>Originally published at </em><a href=\"https://www.linkedin.com/pulse/tame-artificial-intelligence-from-your-laptop-rajat-kumar-pfnae/?trackingId=1JjRp6xcT7ObIWtT6qQ8Hw%3D%3D\"><em>https://www.linkedin.com</em></a><em>.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d91d3d38b9b4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">Tame Artificial Intelligence, from your laptop</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PbGZF4nbTbblAmAijk7Kdw.png\"><figcaption>llamapp beta. (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a>)</figcaption></figure><blockquote>This is a personal project (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a> ) where I explore the extents of layer 2 solutions on top of AI models all run locally — contextually aware, no hallucinations and secure.(Shoutout to <strong>Langchain</strong>(<a href=\"https://github.com/langchain-ai\">https://github.com/langchain-ai</a>), a really cool company enabling the “powered by AI” era. P.S. the actual powered by AI and not just a prompt hiding behind a beautiful UI)</blockquote>\n<p>You know the feeling when you get a new laptop and you immediately want to see what breaks it? No? Maybe it’s me.</p>\n<p>Well turns out, running a super powerful (yet tiny by AI standards) <a href=\"https://ai.google.dev/gemma?trk=article-ssr-frontend-pulse_little-text-block\">Google Gemma</a> , a vector DB (<a href=\"http://redis.io/\">http://redis.io</a>) and a local server doesn’t. And that it gives far better results than expected.</p>\n<h3>The Elevator Pitch</h3>\n<blockquote><em>If you are methodical and pragmatic like me, the accuracy and reliability of AI responses are paramount. Many commercial AI solutions can and do produce hallucinations-answers that seem plausible but are incorrect. Additionally, the concerns about data privacy and security when using cloud-based solutions might make you want to avoid using personal context. To address these issues, I developed the Retrieval Augmented Generator (RAG) application designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</em></blockquote>\n<h3>Why Run Local and What Does It Take?</h3>\n<p>Running AI locally has significant advantages, including enhanced data privacy, control, and speed. Surprisingly, it doesn’t require a high-end setup. I ran this project on a MacBook Air M3, proving that a mid-range laptop is sufficient. This setup ensures that all data stays on your machine, giving you complete control over your information. More importantly, power to tame the AI with context and knowledge from your domain.</p>\n<h3>Understanding RAG in Simple Terms</h3>\n<p>So, what is RAG? Imagine you ask a question and receive an answer that’s not just generated by an AI but backed by verified documents. RAG works in two steps:</p>\n<p><strong>1. Retrieving Relevant Documents: </strong>It searches a database of reliable documents to find information related to your query.</p>\n<p><strong>2. Generating Responses: </strong>Using AI, it crafts a response based on the retrieved information, ensuring the answer is accurate and contextually relevant.</p>\n<p>This approach offers significant benefits over commercial AI apps, which may not always provide traceable answers or may store your data on external servers.</p>\n<p>That is just it in a nutshell — but the extent of RAG is tremendous. Chaining and combining responses with your tools of choice, executing entire workflows even.</p>\n<h3>Key Features of the RAG Engine( I named it Llamapp)</h3>\n<p>Here’s a quick overview of the standout features of the RAG Engine:</p>\n<p><strong>- Rewriting </strong>: Refines user queries to avoid distractions and enhance accuracy.</p>\n<p><strong>- Fusion Reciprocal Ranking </strong>: Combines results from multiple queries to improve response relevance. Think of google page ranking but tuned for you.</p>\n<p><strong>- Custom Loaders and Retrievers </strong>: Ensures the system can handle various document types and retrieve the most pertinent information.</p>\n<p>I am still exploring the need and various ways we can use this. I want to share this project with the community and invite you to contribute, explore, and collaborate. Whether you’re an AI enthusiast, a developer, or someone curious about the potential of local AI, your input and contributions are invaluable.</p>\n<p><strong><em>Check out the project on GitHub: </em></strong><a href=\"https://github.com/rajatasusual/llamapp?trk=article-ssr-frontend-pulse_little-text-block\"><strong><em>[GitHub link]</em></strong></a></p>\n<p>Let’s connect, share insights, and build something amazing together!</p>\n<p>This project is entirely open source, underscoring my commitment to transparency, collaboration, and shared learning. Dive into the code, tweak it, improve it, and let’s push the boundaries of what’s possible with AI!</p>\n<p>#AI #MachineLearning #Langchain #Redis #Ollama #InformationRetrieval #LearningJourney #OpenSource #GitHubProject #TechInnovation #LinkedInArticle</p>\n<p><em>Originally published at </em><a href=\"https://www.linkedin.com/pulse/tame-artificial-intelligence-from-your-laptop-rajat-kumar-pfnae/?trackingId=1JjRp6xcT7ObIWtT6qQ8Hw%3D%3D\"><em>https://www.linkedin.com</em></a><em>.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d91d3d38b9b4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">Tame Artificial Intelligence, from your laptop</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "enclosure": {},
            "categories": [
                "programming",
                "llm",
                "gpt",
                "ai",
                "technology"
            ]
        },
        {
            "title": "LLMs Don’t Always Hold The Golden Ticket",
            "pubDate": "2024-07-18 17:30:13",
            "link": "https://medium.com/taming-the-llama/llms-dont-always-hold-the-golden-ticket-95b4dca23ad0?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/95b4dca23ad0",
            "author": "Rajat",
            "thumbnail": "",
            "description": "\n<figure><img alt=\"Llamapp logo. A cute llama with a blue circle background\" src=\"https://cdn-images-1.medium.com/max/250/1*hj4UEaB4sxHbLIolQcM0-A.png\"><figcaption>llamapp</figcaption></figure><blockquote>👋🏼 Dear tech leaders, do you feel LLMs are the silver bullet for your teams?</blockquote>\n<h3>Q1. How important is accuracy for your team?</h3>\n<p><a href=\"https://arxiv.org/abs/2310.01469\">LLMs hallucinate</a>. Would you want to add another check in your team’s workflows to verify AI responses? Think of LLMs like GPT4 as off the rack suits. They are trained on general data. They try to stitch together the answer with highest statistical match, without traceability or verifiability to back it up. That is why it is called “generative” AI in simpler terms.</p>\n<blockquote>Retraining LLMs to increase accuracy is time and cost intensive process, often still not guaranteeing success.</blockquote>\n<h3>Q2. How sensitive &amp; proprietary is the data you work with?</h3>\n<p>Here’s a thing about LLMs — these smartypants need a ton of computation power, which means they’ve got to use a subprocessor like AWS to help them do their thing. Now, If you’d want to leverage niche Domain specific Knowledge, how would you structurally relay it to LLMs? Assuming you manage that, you’d still need to make peace with the fact that your data doesn’t sit on your machines — it’s quite secure until it’s not.</p>\n<blockquote>If you want to integrate sensitive information from local databases (documents, chats etc) into your workflows, think twice before using public LLMs.</blockquote>\n<h3>Q3. How “generic” is your ambition?</h3>\n<blockquote>“Genius is in the process” — Someone smart.</blockquote>\n<p>If your work is methodical and pragmatic and you’d truly want to leverage the benefits of automation that AI can provide in your workflows, you would be handicapped by LLMs rigid architecture. With limited contextual windows and still primitive functional execution most commercial LLMs provide, you’d end up using it only for “generic” retrieval and creation — Not truly achieving a maturity in your processes.</p>\n<blockquote>If your intention is to create a systemic change in a process intensive organisation, LLMs might prove limiting.</blockquote>\n<h3>Final Thoughts</h3>\n<p>LLMs are heavily trained and seemingly can achieve impossible feats. That’s only because of the sheer amount of “general” data they are trained on. In the end, it’s just math governed by statistical likelihood. So, if your ambition is to integrate your workflows that operate on sensitive or Domain Specific Knowledge (DOKE), you’d want to find an alternative which is not this prone to hallucinations and at the mercy of third-party security.</p>\n<blockquote>There are alternatives. You can still be “AI powered” without slapping prompts to GPT4.</blockquote>\n<h3>What’s next?</h3>\n<p>The alternatives. Just like LLMs, there are MLMs and SLMs which:</p>\n<ol>\n<li>\n<strong>Do not require huge computational power</strong>, thereby can be run on local systems. Look at llama3 or gemma by Google.</li>\n<li>\n<strong>Provide flexibility </strong>to connect into your workflows to more from generative to actionable AI.</li>\n<li>\n<strong>Do not hallucinate</strong>, and can sift through your sensitive databases and catalog of information.</li>\n</ol>\n<blockquote>llamapp (github.com/rajatasusual/llamapp) is an open source project designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</blockquote>\n<p>#AI #MachineLearning #LLM #SLM #OpenAI #Technology #DigitalTransformation</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=95b4dca23ad0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/llms-dont-always-hold-the-golden-ticket-95b4dca23ad0\">LLMs Don’t Always Hold The Golden Ticket</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "content": "\n<figure><img alt=\"Llamapp logo. A cute llama with a blue circle background\" src=\"https://cdn-images-1.medium.com/max/250/1*hj4UEaB4sxHbLIolQcM0-A.png\"><figcaption>llamapp</figcaption></figure><blockquote>👋🏼 Dear tech leaders, do you feel LLMs are the silver bullet for your teams?</blockquote>\n<h3>Q1. How important is accuracy for your team?</h3>\n<p><a href=\"https://arxiv.org/abs/2310.01469\">LLMs hallucinate</a>. Would you want to add another check in your team’s workflows to verify AI responses? Think of LLMs like GPT4 as off the rack suits. They are trained on general data. They try to stitch together the answer with highest statistical match, without traceability or verifiability to back it up. That is why it is called “generative” AI in simpler terms.</p>\n<blockquote>Retraining LLMs to increase accuracy is time and cost intensive process, often still not guaranteeing success.</blockquote>\n<h3>Q2. How sensitive &amp; proprietary is the data you work with?</h3>\n<p>Here’s a thing about LLMs — these smartypants need a ton of computation power, which means they’ve got to use a subprocessor like AWS to help them do their thing. Now, If you’d want to leverage niche Domain specific Knowledge, how would you structurally relay it to LLMs? Assuming you manage that, you’d still need to make peace with the fact that your data doesn’t sit on your machines — it’s quite secure until it’s not.</p>\n<blockquote>If you want to integrate sensitive information from local databases (documents, chats etc) into your workflows, think twice before using public LLMs.</blockquote>\n<h3>Q3. How “generic” is your ambition?</h3>\n<blockquote>“Genius is in the process” — Someone smart.</blockquote>\n<p>If your work is methodical and pragmatic and you’d truly want to leverage the benefits of automation that AI can provide in your workflows, you would be handicapped by LLMs rigid architecture. With limited contextual windows and still primitive functional execution most commercial LLMs provide, you’d end up using it only for “generic” retrieval and creation — Not truly achieving a maturity in your processes.</p>\n<blockquote>If your intention is to create a systemic change in a process intensive organisation, LLMs might prove limiting.</blockquote>\n<h3>Final Thoughts</h3>\n<p>LLMs are heavily trained and seemingly can achieve impossible feats. That’s only because of the sheer amount of “general” data they are trained on. In the end, it’s just math governed by statistical likelihood. So, if your ambition is to integrate your workflows that operate on sensitive or Domain Specific Knowledge (DOKE), you’d want to find an alternative which is not this prone to hallucinations and at the mercy of third-party security.</p>\n<blockquote>There are alternatives. You can still be “AI powered” without slapping prompts to GPT4.</blockquote>\n<h3>What’s next?</h3>\n<p>The alternatives. Just like LLMs, there are MLMs and SLMs which:</p>\n<ol>\n<li>\n<strong>Do not require huge computational power</strong>, thereby can be run on local systems. Look at llama3 or gemma by Google.</li>\n<li>\n<strong>Provide flexibility </strong>to connect into your workflows to more from generative to actionable AI.</li>\n<li>\n<strong>Do not hallucinate</strong>, and can sift through your sensitive databases and catalog of information.</li>\n</ol>\n<blockquote>llamapp (github.com/rajatasusual/llamapp) is an open source project designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</blockquote>\n<p>#AI #MachineLearning #LLM #SLM #OpenAI #Technology #DigitalTransformation</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=95b4dca23ad0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/llms-dont-always-hold-the-golden-ticket-95b4dca23ad0\">LLMs Don’t Always Hold The Golden Ticket</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "enclosure": {},
            "categories": [
                "llm",
                "digital-transformation",
                "ai",
                "technology",
                "gpt"
            ]
        }
    ]
}